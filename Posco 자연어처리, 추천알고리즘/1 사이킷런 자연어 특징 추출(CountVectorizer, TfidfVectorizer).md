# 사이킷런 자연어 특징 추출(CountVectorizer, TfidfVectorizer)

[출처1](https://wiserloner.tistory.com/917) 공부하려고 여기서 퍼왔습니다!

[출처2](https://wiserloner.tistory.com/646)

​     

## 텍스트 특징 추출 대표적 종류

- **CountVectorizer** : 각 텍스트에서 단어 출현 횟수를 카운팅한 벡터
- **TfidfVectorizer** : TF-IDF라는 값을 사용하여 CountVectorizer의 단점을 보완함  [출처2](https://wiserloner.tistory.com/646)
- HashingVectorizer : CountVectorizer에서 해시 함수를 사용하여 속도를 높임

​    

[프로젝트 결론]

결국 줄거리가 비슷한 연관 영화 콘텐츠를 추천해주기 위해

(기존 알던대로 단어 빈도수 기반으로 추천해준 것이 맞긴 하지만)

단순히 단어 빈출 횟수 만을 가지고 판별해 추천해준 것이 아닌

더 고도화된  TF-IDF 알고리즘을 사용하여(결국 줄거리 비슷한 영화를 찾기 더 정확해짐)  줄거리가 비슷한 콘텐츠를 추천해주었다.

---

## 1. CountVectorizer

\- 가장 단순한 특징으로, 텍스트에서 단위별 등장횟수를 카운팅하여 수치벡터화 하는 것입니다.

단위는 정하는대로입니다.

문서 단위, 문장 단위, 단어 단위...

가장 많이 사용되는 것은 단어단위의 카운팅입니다.

​    

---

## 2. 자연어 처리 TF-IDF 개념 정리

### TF-IDF란?

**TF** : Term Frequency의 약자

**1개 문서 안에서 특정 단어의 등장 빈도**를 의미합니다.

문장을 단어로 나누고,

전체 단어수에서 특정 단어가 얼마나 사용되었는지를 파악하여, 해당 문서의 성질을 파악하는 지표로 사용되죠.

단어라고 했는데, 주로 형태소 단위로 나누며,

또한 알파벳의 경우는 1글자 단어의 경우는 생략하는 경우가 많습니다.

**DF** : Document Frequency의 약자

이는 **특정 단어가 나타나는 문서 수**를 나타냅니다.

TF와는 분류 스케일이 달라진 것이라 생각하세요. (단어가 주체인지 문서가 주체인지)

**IDF** : DF에서, 일종의 역수 변환을 해준 값.

```tex
TF-IDF = TF * IDF
```

​    

### TF-IDF를 사용하는 이유

**특징 추출 기법은 여러가지가 있는데, 단순하게는, 단어의 빈도수를 기반으로 특징을 추출하는 CountVectorizer라는 기법이 있습니다. 하지만 이러한 count 기반의 특징 추출은, 조사, 관사처럼 많이 등장하는 단어들을 높게 쳐주기 때문에, 유의미한 결과를 얻기 어렵습니다.**

​     

**TF-IDF는, 그 한계를 해결하고자 발전한 것으로, 단순히 많이 등장하는 빈도수로, 그 단어의 가치를 정하는 것이 아니라, 많이 등장하는 단어들에게는 일종의 패널티를 주어서, 단어 빈도의 스케일을 맞춰주는 기법이라 할수 있습니다.**

**즉, 단어 발생 빈도인 TF에서, 전체 발생 횟수에 따른 패널티를 부여해준 개념이 바로 TF-IDF라고 이해하세요.**

TF-IDF = TF * IDF

(즉, '은', '는' 이러한 관사 조사 같은 경우는 해당 문서에서 큰 빈도로 검출 되었어도, 다른 문서에서도 많이 등장하기 때문에 그의 역값을 곱해(=패널티를 주어) 값을 낮춘다.

반면 a라는 단어가 한 문서에서는 많이 검출되고 다른 문서에서는 검출되지 않았다면, 패널티 주는 값이 더 줄어들어 값을 낮추지 않을 것이다.)

​    

### TF-IDF 알고리즘

먼저 우리는 개념을 알아봤고, 다음에는 제공되는 라이브러리를 이용하여, 결과를 쉽게 얻어냈습니다.

그대로 사용하는 것도 좋지만,

한번이라도 그 알고리즘을 파악하는 것과, 아닌 것은 무척 큰 차이를 가집니다.

​    

- 단어 벡터화

  1. 먼저 vocaburary란 이름의 dict 타입 하나를 만듭니다.

     문서의 문장을, 앞에서부터 확인하며, 형태소 분석 기법에 따라서, 단어들을 추출합니다.(형태소 분석 기법은 여기선 다루지 않겠습니다.)

     문장의 앞에서부터 단어가 추출되면, 각 단어들을, 사전형 안에서, 그 문자열을 기준으로 키값을 하나씩 만듭니다.

     처음 나온 단어는,

     해당 단어로 키값을 만들고, 그에 매칭되는 카운터 값을 1로 초기화합니다.

     ​    

  2. 그렇게 형태소를 추출해나가며, 형태소별로, vocaburary 안에, 동일 이름의 키가 있는지 없는지를 확인합니다.

     동일 키가 없다면, 해당하는 키를 1번 과정처럼, 새 키값으로 만들어서 1값을 대입하고,

     동일 키가 있다면, 해당 키의 기존값에 +1해줍니다.

     

     여기까지가 단어 벡터라이징입니다.

     vocaburary 사전형 데이터가 생성된 것이죠.

- TF 과정

  TF 개념은 미리 설명한 것과 같습니다.

  벡터화랑 알고리즘이 비슷한데, 전체 벡터화와는 달리,

  문서별로 해당 단어가 얼마나 등장하는지를 카운트 하면 됩니다.

  (하나의 문서별 기준으로 하기에, 3 문서의 경우는 3개의 TF가 나오죠.)

- DF 과정

  TF와 비슷합니다.

  그냥 문서에서 해당 단어가 몇개 나왔는지만 확인하면 됩니다.

  (전체 문서들에서 문서의 개수로 파악하기에, N개의 문서라도, 1개의 DF가 나옵니다.)

- IDF 과정

  IDF = ln((1+n) / (1+df)) +1

  와 같은 계산법에 따라서 계산해내면 됩니다.

  위에서 구해놓은 DF를 사용하면 되죠.

- TF-IDF 과정

  재료는 모두 구해졌습니다.

  저는 문재 해결에 있어서 환원주의(복잡한 현상은, 단순한 여러 요소들의 작용으로 이루어지는 것.)를

  적용하길 무척 좋아하는데, 기반이 되는 재료가 갖춰지면, 문제 해결에 더불어,

  다른 문제에의 적용, 응용 개발에도 도움이 됩니다.

  

  TF-IDF = TF * IDF

  이러한 계산법을 사용하면, 이제, 알고리즘은 모두 설명이 된 것입니다.

